{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 01 – Universe Construction\n",
    "\n",
    "This notebook builds the stock universe by loading raw price data, applying basic filters, \n",
    "and saving valid tickers for downstream analysis. It "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 - Import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edzz0\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "import pandas as pd\n",
    "\n",
    "# Helper functions to load price and compute log returns\n",
    "from data_loader import load_price_data\n",
    "from factor_calculations import compute_log_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Load raw wide-format prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_dir = \"../data/raw_prices\"\n",
    "price_wide_raw = load_price_data(price_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial universe: 503 tickers\n"
     ]
    }
   ],
   "source": [
    "# Print initial universe\n",
    "# Reference universe size (503 tickers in original S&P 500 file)\n",
    "original_tickers = set(price_wide_raw.columns)\n",
    "print(f\"Initial universe: {len(original_tickers)} tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 – Clean Tickers and Filter Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weekends\n",
    "price_wide = price_wide_raw[price_wide_raw.index.dayofweek < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate timestamps\n",
    "price_wide = price_wide[~price_wide.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally: Would drop tickers with potentially faulty prices as follows:\n",
    "\n",
    "# Filter tickers where at least 95% of *observed* prices exceed the 1st percentile of all prices\n",
    "# price_threshold = price_wide.stack().quantile(0.01)  # or set manually if desired\n",
    "\n",
    "# valid_price_fraction = (price_wide > price_threshold).sum() / price_wide.notna().sum()\n",
    "# price_wide = price_wide.loc[:, valid_price_fraction >= 0.95]\n",
    "\n",
    "# For this project: Use no price filter at all — rely solely on coverage + ticker list membership\n",
    "# (assumes you're already working off a curated S&P 500 list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage Filter (In-Sample Universe)\n",
    "min_coverage = 0.85 # Set min coverage to 85%\n",
    "trading_days = price_wide.shape[0] # Set trading days to length of price_wide df\n",
    "coverage = price_wide.notna().sum() / trading_days # Compute coverage\n",
    "valid_tickers = coverage[coverage >= min_coverage].index # Filter valid tickers that have >= 85% coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 / 503 tickers passed full cleaning and coverage filter (89.46%)\n"
     ]
    }
   ],
   "source": [
    "# Print universe after coverage filter\n",
    "baseline_count = len(original_tickers) # Set baseline to length of original tickers list\n",
    "valid_count = len(valid_tickers) # Set valid count to length of valid tickers list\n",
    "coverage_pct = valid_count / baseline_count # Compute coverage\n",
    "\n",
    "print(f\"{valid_count} / {baseline_count} tickers passed full cleaning and coverage filter \"\n",
    "      f\"({coverage_pct:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Create and export filtered prices and valid tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final filtered price panel\n",
    "price_filtered = price_wide[valid_tickers]\n",
    "\n",
    "# Export filtered prices as a parquet file\n",
    "price_filtered.to_parquet('../data/processed/price_filtered.parquet')\n",
    "\n",
    "# Save metadata\n",
    "valid_tickers.to_series().to_csv('../data/metadata/universe_tickers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Print Length of price_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3862 number of trading days\n"
     ]
    }
   ],
   "source": [
    "# print max number of trading days\n",
    "print(f\"{len(price_filtered)} number of trading days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Create and export log returns and log prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check prices before taking log\n",
    "if (price_filtered <= 0).any().any():\n",
    "    raise ValueError(\"Filtered price data contains non-positive values. Cannot compute log.\")\n",
    "\n",
    "# Compute daily log returns\n",
    "log_returns = compute_log_returns(price_filtered)\n",
    "\n",
    "# Export log returns as a parquet file\n",
    "log_returns.to_parquet(\"../data/processed/log_returns.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
